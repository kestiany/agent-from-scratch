# Agent From Scratch

> Build an **agent system from first principles** â€” controllable, testable, and evolvable.

This repository documents a week-by-week construction of an Agent kernel without relying on heavyweight frameworks (e.g. LangGraph). By **Week 06**, the system reaches a *closed-loop, reviewable, traceable* execution architecture.

ğŸŒ **Language**: English | [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)

---

## âœ¨ Design Philosophy

* **Kernel-first**: agent capability emerges from a small, explicit execution loop
* **Single Source of Truth**: state transitions are explicit and serializable
* **Plan â†’ Execute â†’ Review**: every task must pass through review
* **Traceable by default**: every run produces a replayable trace
* **Framework-light**: avoid black-box orchestration

---

## ğŸ§  Week 06: What Is Completed

By the end of Week 06, the system achieves:

### 1. Explicit Agent Kernel

Located in `src/agent/kernel.py`

Responsibilities:

* Drive the **Think â†’ Act â†’ Evaluate â†’ Reflect** loop
* Enforce execution boundaries
* Never embed business logic

The kernel is deterministic given `(state, action)`.

---

### 2. Task Planning & Crew Loop

Located in `src/crew` and `src/orchestration/crew_loop.py`

* `Planner` produces a `TaskPlan`
* `Executor` runs subtasks
* `Reviewer` validates outputs
* Loop continues until all subtasks are completed or failed

This replaces LangGraph-style DAGs with a **linear, inspectable control loop**.

---

### 3. Unified State & Schema

Located in `src/schema` and `src/agent/state.py`

* Task status is explicit (`PENDING / IN_PROGRESS / COMPLETED / FAILED`)
* State is serializable
* No hidden side effects

---

### 4. Execution Tracing

Located in `src/orchestration/tracer.py`

Each run records:

* Run start / finish
* Task start / completion
* Review results

This enables:

* Debugging
* Replay
* Future resumability

---
### Week 07 â€” Experience-Aware Runtime

Week 07 introduces **experience as a first-class runtime concept**.

The system now:

* Records structured task experiences
* Extracts behavior patterns from past runs
* Generates advisory knowledge (not hard rules)
* Injects experience-based suggestions into planning â€” optionally

#### Key additions:

* **Experience Store**  
  Persistent, factual records of task executions

* **Pattern Extraction**  
  Identifies repeated failure, inefficiency, or success patterns

* **Advisory Memory**  
  Human-readable, explainable suggestions derived from experience

* **Planner Advisory Injection**  
  Planner may consult advisory memory, but is never forced to follow it

> âš ï¸ The system does **not** self-modify or auto-learn.  
> Experience influences decisions **only through explicit, inspectable context**.

---

## ğŸ“‚ Project Structure

```
src/
â”œâ”€â”€ agent/ # Agent kernel & cognitive steps
â”‚ â”œâ”€â”€ think.py
â”‚ â”œâ”€â”€ action.py
â”‚ â”œâ”€â”€ evaluate.py
â”‚ â”œâ”€â”€ reflect.py
â”‚ â”œâ”€â”€ kernel.py
â”‚ â””â”€â”€ state.py
â”‚
â”œâ”€â”€ crew/ # Planner / Executor / Reviewer roles
â”‚
â”œâ”€â”€ orchestration/ # Execution loop, tracer, runtime glue
â”‚
â”œâ”€â”€ memory/ # Experience store & advisory memory
â”‚
â”œâ”€â”€ schema/ # Typed task, result, experience schemas
â”‚
â”œâ”€â”€ examples/ # Minimal runnable demos
â”‚
â””â”€â”€ main.py # CLI entry point
```

---

## â–¶ï¸ Running an Example

```bash
python ./src/main.py --show-plan "Analyze risks of this system"
```

---

## ğŸ§­ What Comes Next (Week 08+)

* Advisory confidence & decay
* Multi-plan comparison
* Tool execution sandbox
* Controlled self-improvement loops

---

## ğŸ“œ License

MIT
